{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier as MLP\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier as DT\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import roc_curve, auc, roc_auc_score\n",
    "\n",
    "import numpy as np\n",
    "from deap import gp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_function(opt_vars):\n",
    "    '''\n",
    "        eval_function implements the fitness function, i.e, the evaluation metric.\n",
    "        input:\n",
    "            opt_vars (list): A list with the evaluation metrics (string)\n",
    "                options:\n",
    "                    acc: Accuracy\n",
    "                    auc: Area Under ROC Curve\n",
    "                    prec_S: precision of the class 1\n",
    "                    prec_NS: precision of the class 0\n",
    "                    rec_S: recall of the class 1\n",
    "                    rec_NS: recall of the class 0\n",
    "                    f1_S : F1 score of the class 1\n",
    "                    f1_NS : F1 score of the class 0\n",
    "                    TP: True positives number\n",
    "                    TN: True negatives number\n",
    "                    FP: False positives number\n",
    "                    FN: False negatives number\n",
    "                    \n",
    "                    \n",
    "        returns: A lamda function that implements: list_of_metrics = lambda(y_true, y_pred)            \n",
    "    '''\n",
    "    func = []\n",
    "    metr1 = []\n",
    "    metr2 = []\n",
    "    if 'acc' in opt_vars:\n",
    "        func.append(0)\n",
    "    if 'auc' in opt_vars:\n",
    "        func.append(3)\n",
    "    if 'prec_S' in opt_vars:\n",
    "        func.append(1)\n",
    "        metr1.append((0,0))\n",
    "    if 'prec_NS' in opt_vars:\n",
    "        func.append(1)\n",
    "        metr1.append((0,1))\n",
    "    if 'rec_S' in opt_vars:\n",
    "        func.append(1)\n",
    "        metr1.append((1,0))\n",
    "    if 'rec_NS' in opt_vars:\n",
    "        func.append(1)\n",
    "        metr1.append((1,1))\n",
    "    if 'f1_S' in opt_vars:\n",
    "        func.append(1)\n",
    "        metr1.append((2,0))\n",
    "    if 'f1_NS' in opt_vars:\n",
    "        func.append(1)\n",
    "        metr1.append((2,1))\n",
    "    if 'TN' in opt_vars:\n",
    "        func.append(2)\n",
    "        metr2.append(0)\n",
    "    if 'FP' in opt_vars:\n",
    "        func.append(2)\n",
    "        metr2.append(1)\n",
    "    if 'FN' in opt_vars:\n",
    "        func.append(2)\n",
    "        metr2.append(2)\n",
    "    if 'TP' in opt_vars:\n",
    "        func.append(2)\n",
    "        metr2.append(3)\n",
    "    funcs = []\n",
    "    if 3 in func:\n",
    "        w = lambda y_true, y_pred: roc_auc_score(y_true, y_pred)\n",
    "        funcs.append(w)\n",
    "    if 0 in func:\n",
    "        x = lambda y_true, y_pred: accuracy_score(y_true, y_pred)\n",
    "        funcs.append(x)\n",
    "    if 1 in func:\n",
    "        y = lambda y_true, y_pred: [precision_recall_fscore_support(y_true, y_pred)[i][j] for i,j in metr1]\n",
    "        funcs.append(y)\n",
    "    if 2 in func:\n",
    "        z = lambda y_true, y_pred: [confusion_matrix([j[0] for j in y_true], [k[0] for k in y_pred]).ravel()[i] for i in metr2]\n",
    "        funcs.append(z)\n",
    "    final_func = lambda y_true, y_pred: [f(y_true, y_pred) for f in funcs]\n",
    "    return final_func\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subtree(begin, string):\n",
    "    parentesis = 0\n",
    "    end = begin\n",
    "    flag = 0\n",
    "    for char in string[begin:len(string)]:\n",
    "        if char == '(':\n",
    "            flag = 1\n",
    "            parentesis += 1\n",
    "        elif char == ')':\n",
    "            parentesis -= 1\n",
    "        end += 1\n",
    "        if parentesis == 0 and flag == 1:\n",
    "            break\n",
    "    return string[begin:end]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_construction(individual, clf, param, X_train, y_train, X_test, pset):\n",
    "    exp = gp.PrimitiveTree(individual)\n",
    "    string = str(exp)\n",
    "    ind = [i for i in range(len(string)) if string.startswith('F', i)]\n",
    "    if len(ind) == 0:\n",
    "        ind = [0]\n",
    "    features = []\n",
    "    hist = []\n",
    "    temp = []\n",
    "    for i in ind:\n",
    "        subtree = get_subtree(i,string)\n",
    "        if str(subtree) not in hist:\n",
    "            hist.append(str(subtree))\n",
    "            newtree = exp.from_string(subtree, pset)\n",
    "            temp.append(str(newtree))\n",
    "            features.append(gp.compile(newtree, pset))\n",
    "    if len(features) == 0:\n",
    "        features.append(gp.compile(individual, pset))\n",
    "    X_train_new = []\n",
    "    i = 0\n",
    "    #print(temp)\n",
    "    for x in X_train:\n",
    "        X_train_new.append([])\n",
    "        for feature, t in zip(features,temp):\n",
    "            #print(t)\n",
    "            #print(t, feature(*x))\n",
    "            #print(x)\n",
    "            #str(features)\n",
    "            X_train_new[i].append(feature(*x))\n",
    "        i += 1\n",
    "\n",
    "    if clf == 'knn':\n",
    "        classifier = KNeighborsClassifier(n_neighbors=param[0])\n",
    "    elif clf == 'mlp':\n",
    "        classifier = MLP(hidden_layer_sizes=(param[0], ), activation=param[1], max_iter = 200)\n",
    "    elif clf == 'svm':\n",
    "        classifier = SVC(kernel = param[1])\n",
    "    elif clf == 'dt':\n",
    "        classifier = DT()\n",
    "    elif clf == 'nb':\n",
    "        classifier = GaussianNB()\n",
    "    elif clf == 'kmeans':\n",
    "        classifier = KMeans(n_clusters=param[0])\n",
    "\n",
    "    y_train = np.array([j[0] for j in y_train])\n",
    "\n",
    "    X_train_new = np.array(X_train_new).astype(np.float)\n",
    "    try:\n",
    "        classifier.fit(X_train_new, y_train)\n",
    "    except:\n",
    "        return -1\n",
    "    X_test_new = []\n",
    "    i = 0\n",
    "    for x in X_test:\n",
    "        X_test_new.append([])\n",
    "        for feature in features:\n",
    "            #print(x)\n",
    "            X_test_new[i].append(feature(*x))\n",
    "        i += 1\n",
    "    \n",
    "    X_test_new = np.array(X_test_new).astype(np.float)\n",
    "    y_pred = classifier.predict(X_test_new)\n",
    "    y_pred = np.array([[j, int(not(j))] for j in y_pred])\n",
    "\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
